---
title: ""
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
---

````{r include = FALSE}
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning = FALSE)
```
# DATA 621:  HW 4  
David Quarshie - Group 3  

In this assignment we are given a dataset that represents customer records at an auto insurance company. Our goal is to build a logistic regression model to that will predict the probability that a person will crash their car and also estimate the amount of money it will cost if the person is in a crash. 

```{r Libraries}
library('ggplot2')
library('MASS')
library('dplyr')
library('psych')
library('DataExplorer')
library('mice')
library('caret')
library('faraway')
library('stargazer')
library('pROC')
library('stringr')
```  

# 1. Data Exploration  

The data has to be cleaned to take away the '$' and ',' characters from the fields that have them.  

```{r LoadData}
train_url <- 'https://raw.githubusercontent.com/dquarshie89/Data-621/master/insurance_training_data.csv'
train <- read.csv(train_url, header=TRUE)

test_url <- 'https://raw.githubusercontent.com/dquarshie89/Data-621/master/insurance-evaluation-data.csv'
test <- read.csv(test_url, header=TRUE)  

train$INDEX <- NULL
test$INDEX <- NULL
test$TARGET_AMT <- NULL
test$TARGET_FLAG <- NULL
train$TARGET_FLAG <- as.factor(train$TARGET_FLAG)
```  

```{r CleanData}
money_func <- function(vector) {
    i <- unname(sapply(vector, str_replace_all, '[,$]', ''))
    i <- as.numeric(i)
    return(i)
}  

train$INCOME <- money_func(train$INCOME)
train$HOME_VAL <- money_func(train$HOME_VAL)
train$BLUEBOOK <- money_func(train$BLUEBOOK)
train$OLDCLAIM <- money_func(train$OLDCLAIM)

test$INCOME <- money_func(test$INCOME)
test$HOME_VAL <- money_func(test$HOME_VAL)
test$BLUEBOOK <- money_func(test$BLUEBOOK)
test$OLDCLAIM <- money_func(test$OLDCLAIM)
```  



## Basic Statistics

The training data has 8,161 rows and 25 columns or features. Of the 25 columns, 14 are discrete, 11 are continuous, and none are all their missing. We do have 970 missing values out of 204,025 data points.

```{r, echo=FALSE, warning=FALSE}
summary <- describe(train[,c(1:25)])[,c(2:5,8,9,11,12)]
knitr::kable(summary)
```  

##  Distribution of Target Variable

Let's look at the target_flag variable in our training data to make sure there no one sided distribution.  


```{r, echo=FALSE, warning=FALSE}
knitr::kable(table(train$TARGET_FLAG))
sum(train$TARGET_AMT ==0)
```  

## Histogram of Variables

```{r, echo=FALSE, warning=FALSE}
out <- split_columns(train)
plot_histogram(out$continuous)
plot_bar(out$discrete)
```

## Boxplot of Variables

```{r, echo=FALSE, warning=FALSE}
plot_boxplot(
  data = train,
  by = "TARGET_AMT")+ 
  geom_jitter()
```  

# 2. DATA PREPARATION  

## Adjusting Variables  

Looking at the plots we see we have to make a few changes to some variables. We'll make HOMEKIDS boolean instead of a factor. For the rows where CAR_AGE are labled numbers, we make 0. For blank JOBS we label those as "Unknown". Finally, change Education to 1 if PhD and Masters.


```{r Homekids}
train$HOMEKIDS[train$HOMEKIDS != 0 ] <- 1
test$HOMEKIDS[test$HOMEKIDS != 0 ] <- 1
```  

```{r CarAge}
train$CAR_AGE[train$CAR_AGE < 0 ] <- 0
test$CAR_AGE[test$CAR_AGE < 0 ] <- 0
```  

```{r JOB}
train$JOB <- as.character(train$JOB)
train$JOB[train$JOB == ""] <- "Unknown"
train$JOB <- as.factor(train$JOB)
test$JOB <- as.character(test$JOB)
test$JOB[test$JOB == ""] <- "Unknown"
test$JOB <- as.factor(test$JOB)
```

```{r Education}
train$EDUCATION <- ifelse(train$EDUCATION %in% c("PhD", "Masters"), 0, 1)
```


## Missing Data  

We have missing data in income, yoj, home_val, and car_age. 

```{r}
summary(train)
plot_missing(train)
```  

## Imputate  

Let's runmice imputation on both the train and test set.  

```{r}
mice_imputes <- mice(train, m = 2, maxit = 2, print = FALSE)
densityplot(mice_imputes)  
```  


```{r}
m_train <- median(train$AGE, na.rm = T)
train$AGE[is.na(train$AGE)] <- m_train

m_test <- median(train$AGE, na.rm = T)
train$AGE[is.na(train$AGE)] <- m_test

mice_train <-  mice(train, m = 1, maxit = 1, print = FALSE)
train <- complete(mice_train)

mice_test <- mice(test, m = 1, maxit = 1, print = FALSE)
test <- complete(mice_test)
```  

# 3. Build Models  

## Model 1

For the first model we will include all the variables.  Looking at the output of the model we see that some points are highly colinear and a some variables that may not be necessary.   

Model 1 uses the formula:  

__target ~ .__  

```{r}
set.seed(121)
train_logistic <- train
train_logistic$TARGET_AMT <- NULL
split <- createDataPartition(train_logistic$TARGET_FLAG, p=0.85, list=FALSE)
partial_train <- train_logistic[split, ]
validation <- train_logistic[ -split, ]
```


```{r}
mod1 <- train(TARGET_FLAG ~., data = partial_train, 
              method = "glm", family = "binomial",
              trControl = trainControl(
                  method = "cv", number = 10,
                  savePredictions = TRUE),
              tuneLength = 5, 
              preProcess = c("center", "scale"))
```

```{r}
knitr::kable(vif(mod1$finalModel))
```  

## Model 2

For the second model we ignore what's colinear but remove unneccessary variables shown in model 1.    

Model 2 uses the formula: 

__TARGET_FLAG ~ KIDSDRIV + INCOME + PARENT1 + HOME_VAL + MSTATUS + JOB + TRAVTIME + CAR_USE + BLUEBOOK + TIF + CAR_TYPE + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + URBANICITY__

```{r, echo=FALSE, warning=FALSE}
# remove low p-values
mod2 <- train(TARGET_FLAG ~ KIDSDRIV + INCOME + PARENT1 + HOME_VAL +
                  MSTATUS + JOB + TRAVTIME + CAR_USE + BLUEBOOK + TIF + 
                  CAR_TYPE + OLDCLAIM + CLM_FREQ + REVOKED + 
                  MVR_PTS + URBANICITY, 
            data = partial_train, 
            method = "glm", family = "binomial",
            trControl = trainControl(
                  method = "cv", number = 10,
                  savePredictions = TRUE),
            tuneLength = 5, 
            preProcess = c("center", "scale"))
knitr::kable(vif(mod2$finalModel))
```  

## Model 3

Model 3 removes the variables with the 3 highest VIF values from Model 1.     

Model 3 uses the formula:

__TARGET_FLAG ~ KIDSDRIV + AGE + HOMEKIDS + YOJ + INCOME + PARENT1 + HOME_VAL + MSTATUS + JOB + TRAVTIME + CAR_USE + BLUEBOOK + TIF + CAR_TYPE + RED_CAR + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + URBANICITY__

```{r, echo=FALSE, warning=FALSE}
mod3 <- train(TARGET_FLAG ~ KIDSDRIV + AGE + HOMEKIDS + YOJ + 
                  PARENT1 + HOME_VAL + MSTATUS + JOB + 
                  TRAVTIME + CAR_USE + BLUEBOOK + TIF + CAR_TYPE + 
                  RED_CAR + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + 
                  CAR_AGE + URBANICITY, 
              data = partial_train, 
              method = "glm", family = "binomial",
              trControl = trainControl(
                  method = "cv", number = 10,
                  savePredictions = TRUE),
              tuneLength = 5, 
              preProcess = c("center", "scale"))
knitr::kable(vif(mod3$finalModel))
```

## Model 4

Model 4 takes in Model 3 and removes thoe values that are poor predictors.    

Model 4 uses the formula: 

__TARGET_FLAG ~ KIDSDRIV + PARENT1 + HOME_VAL + MSTATUS + JOB + TRAVTIME + CAR_USE + BLUEBOOK + TIF + CAR_TYPE + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + URBANICITY__

```{r, echo=FALSE, warning=FALSE}
## reduce collinearity, and remove low values
mod4 <- train(TARGET_FLAG ~ KIDSDRIV + 
                  PARENT1 + HOME_VAL + MSTATUS + JOB + 
                  TRAVTIME + CAR_USE + BLUEBOOK + TIF + CAR_TYPE + 
                  OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + 
                  CAR_AGE + URBANICITY, 
            data = partial_train, 
            method = "glm", family = "binomial",
            trControl = trainControl(
                  method = "cv", number = 10,
                  savePredictions = TRUE),
            tuneLength = 5, 
            preProcess = c("center", "scale"))
knitr::kable(vif(mod4$finalModel))
```  
## Regression

### Regression Model 1

Regression Model 1 fits includes all the variables.   

__target ~ .__

```{r}
set.seed(121)
train_regression <- train
train_regression <- train_regression[train_regression$TARGET_FLAG == 1, ]
train_regression$TARGET_FLAG <- NULL
mod1lm <- train(TARGET_AMT ~., data = train_regression, 
              method = "lm", 
              trControl = trainControl(
                  method = "cv", number = 10,
                  savePredictions = TRUE),
              tuneLength = 5, 
              preProcess = c("center", "scale"))
```

### Regression Model 2

For Regression Model 2 we take out features with less impact.  

Regression Model 2's formula:  

__TARGET_AMT ~ HOME_VAL +  CAR_USE + BLUEBOOK + TIF + CAR_TYPE + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + URBANICITY__

```{r}
mod2lm <- train(TARGET_AMT ~ HOME_VAL +  
                  CAR_USE + BLUEBOOK + TIF + CAR_TYPE + 
                  OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + 
                  CAR_AGE + URBANICITY, data = train_regression, 
              method = "lm", 
              trControl = trainControl(
                  method = "cv", number = 10,
                  savePredictions = TRUE),
              tuneLength = 5, 
              preProcess = c("center", "scale"))
```

### Regression Model 3

Regression Model 3 deals with the issues related to the car value and driver's legal issues.

Regression Model 3's formula:   

__TARGET_AMT ~ BLUEBOOK + REVOKED + MVR_PTS + CAR_AGE__

```{r}
mod3lm <- train(TARGET_AMT ~ BLUEBOOK + REVOKED + MVR_PTS + 
                  CAR_AGE, data = train_regression, 
              method = "lm", 
              trControl = trainControl(
                  method = "cv", number = 10,
                  savePredictions = TRUE),
              tuneLength = 5, 
              preProcess = c("center", "scale"))
```  

### Regression Model Results
```{r}
df <- data.frame()
df <- rbind(df, mod1lm$results)
df <- rbind(df, mod2lm$results)
df <- rbind(df, mod3lm$results)
df$intercept <- c("Mod1", "Mod2", "Mod3")
colnames(df)[1] <- "model"
knitr::kable(df)
```

# 4. SELECT MODEL  

It's time for us to pick which model we want to use. To help do this we'll review each model's accuracy by making predictions on the 20% we kept and comparing their results. We'll use fourfold plots, summary statistics, and ROC / AUC plots to determine overall accuarcy.  

To aid in model selection for the regression problem, we'll compare the error in the fit of our models in a table and select from there. 

### 1:  Classification  

## Fourfold Plots

```{r, echo=FALSE, warning=FALSE}
preds1 <- predict(mod1, newdata = validation)
preds2 <- predict(mod2, newdata = validation)
preds3 <- predict(mod3, newdata = validation)
preds4 <- predict(mod4, newdata = validation)
m1cM <- confusionMatrix(preds1, validation$TARGET_FLAG, 
                        mode = "everything")
m2cM <- confusionMatrix(preds2, validation$TARGET_FLAG, 
                        mode = "everything")
m3cM <- confusionMatrix(preds3, validation$TARGET_FLAG, 
                        mode = "everything")
m4cM <- confusionMatrix(preds4, validation$TARGET_FLAG, 
                        mode = "everything")
par(mfrow=c(2,2))
fourfoldplot(m1cM$table, color = c("#B22222", "#2E8B57"), main="Mod1")
fourfoldplot(m2cM$table, color = c("#B22222", "#2E8B57"), main="Mod2")
fourfoldplot(m3cM$table, color = c("#B22222", "#2E8B57"), main="Mod3")
fourfoldplot(m4cM$table, color = c("#B22222", "#2E8B57"), main="Mod4")
```

## Summary Statistics

```{r, echo=FALSE, warning=FALSE}
eval <- data.frame(m1cM$byClass, 
                   m2cM$byClass, 
                   m3cM$byClass, 
                   m4cM$byClass)
eval <- data.frame(t(eval))
# manipulate results DF
eval <- dplyr::select(eval, Sensitivity, Specificity, Precision, Recall, F1)
row.names(eval) <- c("Model1", "Model2", "Model3", "Model4")
knitr::kable(eval)
```

### ROC / AUC

```{r, echo=FALSE, warning=FALSE}
getROC <- function(model) {
    name <- deparse(substitute(model))
    pred.prob1 <- predict(model, newdata = train, type="prob")
    p1 <- data.frame(pred = train$TARGET_FLAG, prob = pred.prob1[[1]])
    p1 <- p1[order(p1$prob),]
    rocobj <- roc(p1$pred, p1$prob)
    plot(rocobj, asp=NA, legacy.axes = TRUE, print.auc=TRUE,
         xlab="Specificity", main = name)
}
par(mfrow=c(2,2))
getROC(mod1)
getROC(mod2)
getROC(mod3)
getROC(mod4)
```

### Model Selection - Classification

Our first 2 models have the most information, but they also suffer from colinearity issues as seen by the VIF output. Model 3 performs well, but has some additional variables are poor predictors. So we'll use Model 4.     

Before we make predictions, let's run this final model over our full dataset, and review some summary diagnostic plots and output.  

```{r, echo=FALSE, warning=FALSE}
finalmod <- train(TARGET_FLAG ~ KIDSDRIV + 
                  PARENT1 + HOME_VAL + MSTATUS + JOB + 
                  TRAVTIME + CAR_USE + BLUEBOOK + TIF + CAR_TYPE + 
                  OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + 
                  CAR_AGE + URBANICITY, 
            data = train, 
            method = "glm", family = "binomial",
            trControl = trainControl(
                  method = "cv", number = 10,
                  savePredictions = TRUE),
            tuneLength = 5, 
            preProcess = c("center", "scale"))
summary(finalmod)
plot(finalmod$finalModel)
```  


## Make Predictions

Finally we make our final predictions. We'll create a dataframe with the predictions and the predicted probabilities for our classification.   

For our predictive model we're off on the classification. We'll make a prediction on the target amount for all observations in the test set, regardless of whether we think they'll make a claim.  

```{r, warning=FALSE, message=FALSE}
finalpreds <- predict(finalmod, test)
finalpreds.probs <- predict(finalmod, test, type="prob")
finaldf <- cbind(finalpreds.probs, TARGET_FLAG=finalpreds)
finalAmountPreds <- predict(mod3lm, test)
finaldf <- cbind(finaldf, TARGET_AMT = finalAmountPreds)

write.csv(finaldf, 'HW4predictions.csv', row.names = FALSE)
```

```{r}
knitr::kable(head(finaldf))


knitr::kable(head(finalAmountPreds))
```





